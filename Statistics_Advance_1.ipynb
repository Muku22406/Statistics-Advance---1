{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***Statistics Advance Part 1***"
      ],
      "metadata": {
        "id": "IjF8GvGT9J0j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a random variable in probability theory?\n",
        "\n",
        " - In **probability theory**, a **random variable** is a numerical quantity whose value depends on the outcome of a random phenomenon.\n",
        "\n",
        "There are two main types of random variables:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Discrete Random Variable**\n",
        "\n",
        "* Takes on **countable** values (e.g., integers).\n",
        "* Example: Number of heads when flipping a coin 3 times.\n",
        "\n",
        "  * Possible values: 0, 1, 2, 3\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Continuous Random Variable**\n",
        "\n",
        "* Takes on **uncountably infinite** values within an interval (usually real numbers).\n",
        "* Example: The time it takes for a bus to arrive.\n",
        "\n",
        "  * Possible values: Any real number ‚â• 0\n",
        "\n",
        "---\n",
        "\n",
        "### More Formally:\n",
        "\n",
        "A random variable is a function that assigns a real number to each outcome in a sample space of a random experiment.\n",
        "\n",
        "If:\n",
        "\n",
        "* $\\Omega$ is the sample space (set of all outcomes),\n",
        "* Then a random variable $X$ is a function:\n",
        "\n",
        "  $$\n",
        "  X: \\Omega \\rightarrow \\mathbb{R}\n",
        "  $$\n",
        "\n",
        "---\n",
        "\n",
        "### Example:\n",
        "\n",
        "Suppose you roll a die. Let $X$ be the number shown on the die.\n",
        "\n",
        "* $\\Omega = \\{1, 2, 3, 4, 5, 6\\}$\n",
        "* $X(\\omega) = \\omega$, so $X$ is the identity function here.\n",
        "* $X$ is a **discrete random variable**.\n",
        "\n"
      ],
      "metadata": {
        "id": "qhtjsL4H9QaD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What are the types of random variables?\n",
        "\n",
        "- Random variables are generally classified into **two main types** based on the kind of values they can take:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Discrete Random Variable**\n",
        "\n",
        "* **Definition**: Takes on a **finite or countably infinite** set of distinct values.\n",
        "* **Values**: Typically integers or isolated points.\n",
        "* **Examples**:\n",
        "\n",
        "  * Number of students in a classroom.\n",
        "  * Number of heads in 10 coin tosses.\n",
        "  * Number of defective items in a batch.\n",
        "\n",
        "#### Key Characteristics:\n",
        "\n",
        "* Probability is assigned to **individual values**.\n",
        "* Represented using a **probability mass function (PMF)**.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Continuous Random Variable**\n",
        "\n",
        "* **Definition**: Takes on an **uncountable** number of possible values, usually over an interval of real numbers.\n",
        "* **Values**: Any value within a given range (e.g., all real numbers between 0 and 1).\n",
        "* **Examples**:\n",
        "\n",
        "  * Height of a person.\n",
        "  * Time taken to complete a task.\n",
        "  * Temperature at noon.\n",
        "\n",
        "#### Key Characteristics:\n",
        "\n",
        "* Probability is assigned over **intervals**, not individual points (since the probability of a single value is 0).\n",
        "* Represented using a **probability density function (PDF)**.\n",
        "* The **area under the PDF curve** over an interval gives the probability.\n",
        "\n",
        "---\n",
        "\n",
        "### Bonus: Mixed Random Variables (Less Common)\n",
        "\n",
        "* Have both **discrete and continuous** components.\n",
        "* Example: A distribution that has a probability mass at a point (like 0), and a continuous density elsewhere.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary Table:\n",
        "\n",
        "| Type       | Values Taken                   | Probability Function               | Example                        |\n",
        "| ---------- | ------------------------------ | ---------------------------------- | ------------------------------ |\n",
        "| Discrete   | Countable (finite or infinite) | PMF (Probability Mass Function)    | Number of phone calls per hour |\n",
        "| Continuous | Uncountable (intervals)        | PDF (Probability Density Function) | Weight of a person             |\n",
        "\n"
      ],
      "metadata": {
        "id": "zWS7wicB9opy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is the difference between discrete and continuous distributions?\n",
        "\n",
        " - The **difference between discrete and continuous distributions** lies in the type of values the random variable can take and how probability is assigned to those values.\n",
        "\n",
        "Here‚Äôs a detailed comparison:\n",
        "\n",
        "---\n",
        "\n",
        "### üî¢ 1. **Discrete Distributions**\n",
        "\n",
        "* **Definition**: Deal with **discrete random variables**, which take **countable** values.\n",
        "* **Values**: Finite or countably infinite set (e.g., 0, 1, 2, 3, ...).\n",
        "* **Probability Assignment**: Probability is assigned to **individual values**.\n",
        "* **Probability Function**: Uses a **Probability Mass Function (PMF)**.\n",
        "\n",
        "  $$\n",
        "  P(X = x) = p(x)\n",
        "  $$\n",
        "* **Total Probability**: Sum over all possible values is 1.\n",
        "\n",
        "  $$\n",
        "  \\sum_x p(x) = 1\n",
        "  $$\n",
        "\n",
        "#### üßÆ Examples:\n",
        "\n",
        "* Number of heads in 5 coin flips (Binomial distribution)\n",
        "* Number of emails received per hour (Poisson distribution)\n",
        "\n",
        "---\n",
        "\n",
        "### üìà 2. **Continuous Distributions**\n",
        "\n",
        "* **Definition**: Deal with **continuous random variables**, which take **uncountably infinite** values over an interval.\n",
        "* **Values**: Any real number within a range (e.g., 0 ‚â§ x ‚â§ 1).\n",
        "* **Probability Assignment**: Probability is assigned over **intervals**, not individual points.\n",
        "* **Probability Function**: Uses a **Probability Density Function (PDF)**.\n",
        "\n",
        "  $$\n",
        "  P(a \\le X \\le b) = \\int_a^b f(x) \\, dx\n",
        "  $$\n",
        "\n",
        "  And $P(X = x) = 0$ for any exact $x$.\n",
        "* **Total Probability**: The area under the PDF curve is 1.\n",
        "\n",
        "  $$\n",
        "  \\int_{-\\infty}^{\\infty} f(x) \\, dx = 1\n",
        "  $$\n",
        "\n",
        "#### üìä Examples:\n",
        "\n",
        "* Heights of people (Normal distribution)\n",
        "* Time between events (Exponential distribution)\n",
        "\n"
      ],
      "metadata": {
        "id": "PUDRbQ6--C0q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What are probability distribution functions (PDF)?\n",
        "\n",
        "- A **Probability Distribution Function (PDF)** describes how the probabilities are distributed over the values of a **random variable**. The term ‚ÄúPDF‚Äù most commonly refers to **continuous random variables**, but it‚Äôs helpful to understand the full picture.\n",
        "\n",
        "There are **three main types** of probability distribution functions:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Probability Mass Function (PMF)** ‚Äì for **Discrete** Random Variables\n",
        "\n",
        "* **Definition**: A function that gives the **probability** that a discrete random variable equals a specific value.\n",
        "\n",
        "* **Notation**:\n",
        "\n",
        "  $$\n",
        "  P(X = x) = p(x)\n",
        "  $$\n",
        "\n",
        "* **Properties**:\n",
        "\n",
        "  * $0 \\leq p(x) \\leq 1$\n",
        "  * $\\sum_{x} p(x) = 1$\n",
        "\n",
        "* **Example**: Tossing a fair die\n",
        "  $p(1) = p(2) = \\dots = p(6) = \\frac{1}{6}$\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Probability Density Function (PDF)** ‚Äì for **Continuous** Random Variables\n",
        "\n",
        "* **Definition**: A function $f(x)$ such that the **area under the curve** between two values gives the **probability** that the variable falls in that interval.\n",
        "\n",
        "* **Notation**:\n",
        "\n",
        "  $$\n",
        "  P(a \\leq X \\leq b) = \\int_a^b f(x) \\, dx\n",
        "  $$\n",
        "\n",
        "* **Important Note**:\n",
        "\n",
        "  $$\n",
        "  P(X = x) = 0 \\quad \\text{for any specific } x\n",
        "  $$\n",
        "\n",
        "* **Properties**:\n",
        "\n",
        "  * $f(x) \\geq 0$\n",
        "  * $\\int_{-\\infty}^{\\infty} f(x) \\, dx = 1$\n",
        "\n",
        "* **Example**: The **standard normal distribution** has PDF:\n",
        "\n",
        "  $$\n",
        "  f(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2}\n",
        "  $$\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Cumulative Distribution Function (CDF)** ‚Äì for **All** Random Variables\n",
        "\n",
        "* **Definition**: The probability that a random variable $X$ is **less than or equal to** a value $x$.\n",
        "* **Notation**:\n",
        "\n",
        "  $$\n",
        "  F(x) = P(X \\leq x)\n",
        "  $$\n",
        "* **For Discrete**:\n",
        "\n",
        "  $$\n",
        "  F(x) = \\sum_{t \\leq x} p(t)\n",
        "  $$\n",
        "* **For Continuous**:\n",
        "\n",
        "  $$\n",
        "  F(x) = \\int_{-\\infty}^{x} f(t) \\, dt\n",
        "  $$\n",
        "\n"
      ],
      "metadata": {
        "id": "Gs6FeUck-l8r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. How do cumulative distribution functions (CDF) differ from probability distribution functions (PDF)?\n",
        "\n",
        "- Great question! The **Cumulative Distribution Function (CDF)** and the **Probability Distribution Function (PDF)** (or **PMF** for discrete variables) are closely related but serve different purposes. Here's how they differ:\n",
        "\n",
        "---\n",
        "\n",
        "## üìä 1. **Definition & Purpose**\n",
        "\n",
        "| Concept          | **PDF / PMF**                                                                         | **CDF**                                                                         |\n",
        "| ---------------- | ------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------- |\n",
        "| **What it does** | Describes **how** probability is distributed at a point or over an interval           | Describes the **total probability accumulated up to a value**                   |\n",
        "| **Gives you**    | The likelihood of a specific value (discrete) or probability **density** (continuous) | The probability that the variable is **less than or equal to** a specific value |\n",
        "\n",
        "---\n",
        "\n",
        "## üî¢ 2. **Applies To**\n",
        "\n",
        "* **PDF**: Used for **continuous** random variables.\n",
        "* **PMF**: Used for **discrete** random variables.\n",
        "* **CDF**: Used for **both** types.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è 3. **Formulas**\n",
        "\n",
        "### For Discrete Random Variable:\n",
        "\n",
        "* **PMF**:\n",
        "\n",
        "  $$\n",
        "  P(X = x) = p(x)\n",
        "  $$\n",
        "* **CDF**:\n",
        "\n",
        "  $$\n",
        "  F(x) = P(X \\leq x) = \\sum_{t \\leq x} p(t)\n",
        "  $$\n",
        "\n",
        "### For Continuous Random Variable:\n",
        "\n",
        "* **PDF**:\n",
        "\n",
        "  $$\n",
        "  f(x), \\quad \\text{where } P(a \\leq X \\leq b) = \\int_a^b f(x)\\,dx\n",
        "  $$\n",
        "* **CDF**:\n",
        "\n",
        "  $$\n",
        "  F(x) = P(X \\leq x) = \\int_{-\\infty}^x f(t)\\,dt\n",
        "  $$\n",
        "\n",
        "---\n",
        "\n",
        "## üîç 4. **Key Differences**\n",
        "\n",
        "| Feature                    | PDF / PMF                                                        | CDF                                              |\n",
        "| -------------------------- | ---------------------------------------------------------------- | ------------------------------------------------ |\n",
        "| **Type of Function**       | Density (continuous) or mass (discrete)                          | Cumulative sum or integral                       |\n",
        "| **Probability at a point** | PDF: not meaningful (probability at a point is 0 for continuous) | CDF gives total probability **up to** that point |\n",
        "| **Monotonicity**           | Not necessarily increasing                                       | Always **non-decreasing**                        |\n",
        "| **Range**                  | PMF: \\[0, 1] (discrete values)                                   |                                                  |\n",
        "| PDF: $\\geq 0$ (densities)  | CDF: \\[0, 1]                                                     |                                                  |\n",
        "\n",
        "---\n",
        "\n",
        "## üìà 5. **Example (Continuous Case: Normal Distribution)**\n",
        "\n",
        "* **PDF**: Bell-shaped curve showing **density** around the mean.\n",
        "* **CDF**: S-shaped curve starting from 0 and asymptotically approaching 1 as $x \\to \\infty$.\n",
        "\n",
        "---\n",
        "\n",
        "### üîÅ Relationship:\n",
        "\n",
        "* You can get the **CDF from the PDF**:\n",
        "\n",
        "  $$\n",
        "  F(x) = \\int_{-\\infty}^x f(t)\\,dt\n",
        "  $$\n",
        "* And the **PDF from the CDF** (if differentiable):\n",
        "\n",
        "  $$\n",
        "  f(x) = \\frac{d}{dx}F(x)\n",
        "  $$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vpz_e6iECOaj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.  What is a discrete uniform distribution?\n",
        "\n",
        " - A **Discrete Uniform Distribution** is one of the simplest probability distributions in statistics. It describes a situation where a **finite number of outcomes** are **equally likely**.\n",
        "\n",
        "---\n",
        "\n",
        "## üìò **Definition**\n",
        "\n",
        "A **discrete uniform distribution** is a probability distribution where each of the $n$ possible outcomes has the **same probability**.\n",
        "\n",
        "If a random variable $X$ can take on $n$ distinct values $x_1, x_2, \\dots, x_n$, then:\n",
        "\n",
        "$$\n",
        "P(X = x_i) = \\frac{1}{n} \\quad \\text{for all } i = 1, 2, \\dots, n\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## üé≤ **Example**\n",
        "\n",
        "### Rolling a Fair 6-Sided Die:\n",
        "\n",
        "* Possible outcomes: $\\{1, 2, 3, 4, 5, 6\\}$\n",
        "* Each value has a probability of:\n",
        "\n",
        "$$\n",
        "P(X = x) = \\frac{1}{6} \\quad \\text{for } x \\in \\{1, 2, 3, 4, 5, 6\\}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## üìä **Properties**\n",
        "\n",
        "### 1. **Probability Mass Function (PMF)**:\n",
        "\n",
        "$$\n",
        "P(X = x) = \\begin{cases}\n",
        "\\frac{1}{n} & \\text{if } x \\in \\{x_1, x_2, \\dots, x_n\\} \\\\\n",
        "0 & \\text{otherwise}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "### 2. **Mean (Expected Value)**:\n",
        "\n",
        "$$\n",
        "E[X] = \\frac{x_1 + x_n}{2}\n",
        "$$\n",
        "\n",
        "### 3. **Variance**:\n",
        "\n",
        "If $X \\in \\{a, a+1, \\dots, b\\}$, then:\n",
        "\n",
        "$$\n",
        "\\text{Var}(X) = \\frac{(b - a + 1)^2 - 1}{12}\n",
        "$$\n",
        "\n"
      ],
      "metadata": {
        "id": "0lYLjBFNCgVC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What are the key properties of a Bernoulli distribution?\n",
        "\n",
        "- The **Bernoulli distribution** is one of the most fundamental discrete probability distributions in statistics and probability theory. It models a **single trial** (experiment) that has exactly **two possible outcomes**:\n",
        "\n",
        "* **Success** (usually coded as 1)\n",
        "* **Failure** (usually coded as 0)\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ **Key Properties of the Bernoulli Distribution**\n",
        "\n",
        "### 1. **Definition**\n",
        "\n",
        "A random variable $X$ follows a **Bernoulli distribution** with parameter $p$ if:\n",
        "\n",
        "$$\n",
        "P(X = 1) = p \\quad \\text{and} \\quad P(X = 0) = 1 - p\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $p$ is the **probability of success** (0 ‚â§ $p$ ‚â§ 1)\n",
        "* $X \\in \\{0, 1\\}$\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Probability Mass Function (PMF)**:\n",
        "\n",
        "$$\n",
        "P(X = x) = p^x (1 - p)^{1 - x} \\quad \\text{for } x \\in \\{0, 1\\}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Mean (Expected Value)**:\n",
        "\n",
        "$$\n",
        "E[X] = p\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Variance**:\n",
        "\n",
        "$$\n",
        "\\text{Var}(X) = p(1 - p)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Skewness**:\n",
        "\n",
        "$$\n",
        "\\text{Skewness} = \\frac{1 - 2p}{\\sqrt{p(1 - p)}}\n",
        "$$\n",
        "\n",
        "* Skewed right if $p < 0.5$\n",
        "* Skewed left if $p > 0.5$\n",
        "* Symmetric if $p = 0.5$\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **Kurtosis**:\n",
        "\n",
        "$$\n",
        "\\text{Excess Kurtosis} = \\frac{1 - 6p(1 - p)}{p(1 - p)}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### 7. **Support (Values It Can Take)**:\n",
        "\n",
        "$$\n",
        "X \\in \\{0, 1\\}\n",
        "$$\n",
        "\n"
      ],
      "metadata": {
        "id": "AZRKIdpyCySa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the binomial distribution, and how is it used in probability?\n",
        "\n",
        "- The **binomial distribution** is a **discrete probability distribution** that models the number of **successes** in a fixed number of **independent Bernoulli trials**, each with the same probability of success.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ **Definition**\n",
        "\n",
        "A random variable $X$ follows a **Binomial distribution** if:\n",
        "\n",
        "* There are $n$ independent trials\n",
        "* Each trial results in either **success (1)** or **failure (0)**\n",
        "* The probability of success in each trial is $p$\n",
        "* The probability of failure is $1 - p$\n",
        "\n",
        "We write:\n",
        "\n",
        "$$\n",
        "X \\sim \\text{Binomial}(n, p)\n",
        "$$\n",
        "\n",
        "Where $X$ represents the **number of successes** in $n$ trials.\n",
        "\n",
        "---\n",
        "\n",
        "## üìä **Probability Mass Function (PMF)**\n",
        "\n",
        "$$\n",
        "P(X = k) = \\binom{n}{k} p^k (1 - p)^{n - k}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $\\binom{n}{k} = \\frac{n!}{k!(n-k)!}$ is the number of ways to choose $k$ successes from $n$ trials\n",
        "* $k = 0, 1, 2, \\dots, n$\n",
        "\n",
        "---\n",
        "\n",
        "## üìê **Key Properties**\n",
        "\n",
        "| Property     | Formula                                                  |\n",
        "| ------------ | -------------------------------------------------------- |\n",
        "| **Mean**     | $E[X] = np$                                              |\n",
        "| **Variance** | $\\text{Var}(X) = np(1 - p)$                              |\n",
        "| **Mode**     | $\\lfloor (n+1)p \\rfloor$ or $\\lfloor (n+1)p \\rfloor - 1$ |\n",
        "| **Skewness** | $\\frac{1 - 2p}{\\sqrt{np(1-p)}}$                          |\n",
        "| **Support**  | $X \\in \\{0, 1, 2, \\dots, n\\}$                            |\n",
        "\n",
        "---\n",
        "\n",
        "## üìò **Example Use Case**\n",
        "\n",
        "Suppose you flip a fair coin 10 times (so $n = 10$, $p = 0.5$). Let $X$ be the number of heads.\n",
        "\n",
        "* $X \\sim \\text{Binomial}(10, 0.5)$\n",
        "* $P(X = 5) = \\binom{10}{5} (0.5)^5 (0.5)^5 = 0.246$\n",
        "\n",
        "---\n",
        "\n",
        "## üîç **Applications of the Binomial Distribution**\n",
        "\n",
        "* Quality control: Number of defective items in a batch\n",
        "* Marketing: Number of users who click on an ad\n",
        "* Medicine: Number of patients responding to a treatment\n",
        "* Sports: Number of successful free throws in basketball\n",
        "\n",
        "---\n",
        "\n",
        "## üß† **When to Use the Binomial Distribution**\n",
        "\n",
        "Use the binomial distribution when:\n",
        "\n",
        "1. There are a **fixed number** of trials, $n$\n",
        "2. Each trial is **independent**\n",
        "3. Each trial has only **two outcomes** (success/failure)\n",
        "4. The **probability of success** $p$ is the **same** in each trial\n",
        "\n"
      ],
      "metadata": {
        "id": "Y49ESeyIDH7b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is the Poisson distribution and where is it applied?\n",
        "\n",
        " - The **Poisson distribution** is a **discrete probability distribution** that models the number of times an event occurs in a **fixed interval of time or space**, under the assumption that:\n",
        "\n",
        "1. The events occur **independently**.\n",
        "2. The **average rate** (events per unit) is **constant**.\n",
        "3. Two events **cannot occur at the exact same instant**.\n",
        "\n",
        "---\n",
        "\n",
        "## üìò **Definition**\n",
        "\n",
        "A random variable $X$ follows a **Poisson distribution** with parameter $\\lambda$ (lambda), where:\n",
        "\n",
        "* $\\lambda > 0$ is the **average number of events** in a given interval (mean rate of occurrence).\n",
        "\n",
        "We write:\n",
        "\n",
        "$$\n",
        "X \\sim \\text{Poisson}(\\lambda)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## üìä **Probability Mass Function (PMF)**\n",
        "\n",
        "$$\n",
        "P(X = k) = \\frac{e^{-\\lambda} \\lambda^k}{k!}, \\quad \\text{for } k = 0, 1, 2, \\dots\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $e$ ‚âà 2.718 is Euler‚Äôs number\n",
        "* $k$ is the number of occurrences\n",
        "\n",
        "---\n",
        "\n",
        "## üìê **Key Properties**\n",
        "\n",
        "| Property     | Formula                    |\n",
        "| ------------ | -------------------------- |\n",
        "| **Mean**     | $E[X] = \\lambda$           |\n",
        "| **Variance** | $\\text{Var}(X) = \\lambda$  |\n",
        "| **Skewness** | $\\frac{1}{\\sqrt{\\lambda}}$ |\n",
        "| **Kurtosis** | $\\frac{1}{\\lambda}$        |\n",
        "| **Support**  | $X \\in \\{0, 1, 2, \\dots\\}$ |\n",
        "\n",
        "---\n",
        "\n",
        "## üß† **When to Use the Poisson Distribution**\n",
        "\n",
        "Use the Poisson distribution when:\n",
        "\n",
        "* You're counting **how many times an event happens** in a fixed interval (time, area, volume).\n",
        "* Events happen **independently** and **randomly**.\n",
        "* The **rate** $\\lambda$ is **constant** over the interval.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ **Common Applications**\n",
        "\n",
        "| Field           | Example                                         |\n",
        "| --------------- | ----------------------------------------------- |\n",
        "| Call centers    | Number of calls received per hour               |\n",
        "| Biology         | Number of mutations in a DNA strand segment     |\n",
        "| Traffic         | Number of cars passing a checkpoint in 1 minute |\n",
        "| Quality control | Number of defects per square meter of fabric    |\n",
        "| Astronomy       | Number of stars in a given area of sky          |\n",
        "\n",
        "---\n",
        "\n",
        "## üîÅ **Relation to Other Distributions**\n",
        "\n",
        "* The **Poisson distribution** can be derived as a **limit of the Binomial distribution** when:\n",
        "\n",
        "  * $n \\to \\infty$\n",
        "  * $p \\to 0$\n",
        "  * such that $np = \\lambda$ remains fixed\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KvpQd46NDXyq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What is a continuous uniform distribution?\n",
        "\n",
        " - A **continuous uniform distribution** is a probability distribution where all outcomes within a specific interval $[a, b]$ are equally likely. Unlike discrete distributions, this applies to continuous variables, meaning any value between $a$ and $b$ can occur with the same likelihood. Its probability density function (PDF) is constant across the interval, given by $\\frac{1}{b - a}$, and zero outside it. The distribution‚Äôs cumulative distribution function (CDF) increases linearly from 0 at $a$ to 1 at $b$. The mean of this distribution is the midpoint $\\frac{a + b}{2}$, and the variance, which measures how spread out values are, is $\\frac{(b - a)^2}{12}$. This distribution is commonly used when modeling situations where every outcome in a range is equally probable, such as the random arrival time of a bus within a 30-minute window.\n"
      ],
      "metadata": {
        "id": "ayUo9tmdEAVi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What are the characteristics of a normal distribution?\n",
        "\n",
        " - The **normal distribution**, also known as the **Gaussian distribution**, is a continuous probability distribution that is symmetric and bell-shaped. Its key characteristics include being completely defined by two parameters: the **mean** (Œº), which determines the center or peak of the distribution, and the **standard deviation** (œÉ), which measures the spread or dispersion around the mean. The distribution is symmetric about the mean, meaning the left and right sides are mirror images. Most of the data values lie close to the mean, with probabilities tapering off smoothly as you move further away, following the empirical rule: about 68% of data falls within one standard deviation, 95% within two, and 99.7% within three. The normal distribution has a continuous probability density function characterized by the equation $f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$, where $x$ can take any real value. It is widely used in statistics because many natural phenomena approximate this distribution, and it forms the foundation for many statistical methods due to properties like the Central Limit Theorem.\n"
      ],
      "metadata": {
        "id": "1KhFF1u1ESCT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is the standard normal distribution, and why is it important?\n",
        "\n",
        "- The **standard normal distribution** is a special case of the normal distribution with a **mean of 0** and a **standard deviation of 1**. It's often denoted by $Z \\sim N(0, 1)$. The shape is the classic symmetric bell curve centered at zero, and it describes how values deviate from the mean in terms of standard deviations, or \"z-scores.\"\n",
        "\n",
        "Its importance lies in the fact that **any normal distribution can be transformed into the standard normal distribution** by subtracting the mean and dividing by the standard deviation, using the formula:\n",
        "\n",
        "$$\n",
        "Z = \\frac{X - \\mu}{\\sigma}\n",
        "$$\n",
        "\n",
        "This process, called **standardization**, allows statisticians to use universal tables (Z-tables) and tools to find probabilities and percentiles for any normal distribution without recalculating for different means and standard deviations. It simplifies hypothesis testing, confidence intervals, and many other statistical procedures, making the standard normal distribution a foundational tool in probability and statistics.\n"
      ],
      "metadata": {
        "id": "tT6v5m6uE267"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What is the Central Limit Theorem (CLT), and why is it critical in statistics?\n",
        "\n",
        " -  The **Central Limit Theorem (CLT)** is a fundamental concept in statistics that states that when you take sufficiently large samples from any population‚Äîregardless of its original distribution‚Äîthe **distribution of the sample means will approximate a normal distribution**. This holds true even if the population itself is not normally distributed. More formally, as the sample size $n$ grows, the sampling distribution of the mean approaches a normal distribution with mean equal to the population mean and variance equal to the population variance divided by $n$.\n",
        "\n",
        "The CLT is critical because it justifies the widespread use of the normal distribution in statistical inference. It allows us to make reliable conclusions about population parameters using sample data, even when we don‚Äôt know the underlying population distribution. This theorem underpins many statistical methods such as confidence intervals and hypothesis testing, making it possible to apply normal-based techniques broadly and with confidence. Without the CLT, much of classical statistics would be far more complicated or limited in application.\n"
      ],
      "metadata": {
        "id": "_4QqpqKrFEuS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. How does the Central Limit Theorem relate to the normal distribution?\n",
        "\n",
        "- The **Central Limit Theorem (CLT)** directly connects to the **normal distribution** by explaining why the normal distribution appears so frequently in statistics. Specifically, the CLT states that the distribution of the **sample means**‚Äîcalculated from many independent samples of a sufficiently large size taken from any population‚Äîwill tend to follow a **normal distribution**, regardless of the shape of the original population‚Äôs distribution. This means that even if the original data is skewed, uniform, or otherwise non-normal, the averages of large samples will be approximately normally distributed.\n",
        "\n",
        "This relationship is crucial because it allows statisticians to use the properties and tools of the normal distribution (like z-scores, confidence intervals, and hypothesis tests) to analyze sample data and make inferences about the population. In short, the CLT explains why the normal distribution is a cornerstone in statistics, providing a theoretical foundation for treating sample means as normally distributed in many practical scenarios.\n"
      ],
      "metadata": {
        "id": "gaXnhDZuFRNC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. What is the application of Z statistics in hypothesis testing?\n",
        "\n",
        "- **Z statistics** (or **Z-scores**) are widely used in hypothesis testing to determine how far a sample statistic (like a sample mean) is from the hypothesized population parameter, measured in terms of standard deviations. When you know the population standard deviation and the sample size is large (usually $n \\geq 30$), you can use the Z statistic to test hypotheses about population means or proportions.\n",
        "\n",
        "In hypothesis testing, the Z statistic helps you:\n",
        "\n",
        "1. **Calculate the test statistic:** You standardize your sample estimate by subtracting the hypothesized value and dividing by the standard error, producing a Z-score that measures how extreme your sample result is under the null hypothesis.\n",
        "\n",
        "2. **Compare to critical values:** You compare the calculated Z statistic to critical values from the standard normal distribution (Z-table) corresponding to your chosen significance level (e.g., 0.05). This tells you whether to reject or fail to reject the null hypothesis.\n",
        "\n",
        "3. **Find p-values:** The Z statistic helps find the p-value, which quantifies the probability of observing a test statistic as extreme as (or more extreme than) the one calculated, assuming the null hypothesis is true.\n",
        "\n"
      ],
      "metadata": {
        "id": "NNsb_8w3FcOK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. How do you calculate a Z-score, and what does it represent?\n",
        "\n",
        "- A **Z-score** is calculated by taking a value from a dataset, subtracting the mean of the dataset or population, and then dividing the result by the standard deviation. The formula is:\n",
        "\n",
        "$$\n",
        "Z = \\frac{X - \\mu}{\\sigma}\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "* $X$ is the value you're standardizing,\n",
        "* $\\mu$ is the mean of the population (or sample mean if population mean is unknown),\n",
        "* $\\sigma$ is the standard deviation of the population (or sample standard deviation if population SD is unknown).\n",
        "\n",
        "The Z-score represents **how many standard deviations a particular value $X$ is away from the mean**. A positive Z-score means the value is above the mean, while a negative Z-score means it is below the mean. For example, a Z-score of 2 means the value is 2 standard deviations above the mean.\n",
        "\n",
        "Z-scores are useful because they standardize different data points onto a common scale, allowing comparison across different datasets or distributions. They are also fundamental in calculating probabilities and critical values in the context of the normal distribution, especially in hypothesis testing and confidence interval estimation.\n"
      ],
      "metadata": {
        "id": "0q4JupimFnSa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What are point estimates and interval estimates in statistics?\n",
        "\n",
        "- **Point estimates** and **interval estimates** are two ways to estimate unknown population parameters based on sample data.\n",
        "\n",
        "A **point estimate** is a single value calculated from the sample that serves as the best guess or approximation of an unknown population parameter. For example, the sample mean $\\bar{x}$ is a point estimate of the population mean $\\mu$. Point estimates are simple and easy to compute, but they don‚Äôt convey any information about the uncertainty or variability in the estimate.\n",
        "\n",
        "An **interval estimate**, on the other hand, provides a range of values within which the population parameter is believed to lie, along with a certain level of confidence. This is often called a **confidence interval**. For example, a 95% confidence interval for the population mean might be $\\bar{x} \\pm z^* \\times \\frac{\\sigma}{\\sqrt{n}}$, where $z^*$ corresponds to the desired confidence level. Interval estimates give more information than point estimates because they reflect the uncertainty due to sampling variability.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sLmbnNb3F0Gq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What is the significance of confidence intervals in statistical analysis?\n",
        "\n",
        "- Confidence intervals are significant in statistical analysis because they provide a **range of plausible values** for an unknown population parameter, rather than just a single point estimate. This range reflects the **uncertainty inherent in sampling** and helps quantify how confident we can be that the interval contains the true parameter.\n",
        "\n",
        "Unlike a point estimate, a confidence interval acknowledges variability and sampling error, giving a more informative picture of the estimate's precision. For example, a 95% confidence interval means that if we were to take many samples and compute such intervals repeatedly, about 95% of those intervals would contain the true population parameter.\n",
        "\n",
        "This concept is crucial for making **informed decisions** and **drawing conclusions** in research because it shows the reliability of the estimates and helps avoid overconfidence. Confidence intervals are widely used in hypothesis testing, estimation, and reporting results in fields ranging from medicine to social sciences, providing a clearer understanding of data and uncertainty.\n"
      ],
      "metadata": {
        "id": "qq_0SgszGCLa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. What is the relationship between a Z-score and a confidence interval?\n",
        "\n",
        "- The relationship between a **Z-score** and a **confidence interval** lies in how the Z-score helps determine the **width of the confidence interval** around a sample estimate.\n",
        "\n",
        "When constructing a confidence interval for a population mean (assuming a normal distribution and known population standard deviation), the formula is:\n",
        "\n",
        "$$\n",
        "\\text{Confidence Interval} = \\bar{x} \\pm Z^* \\times \\frac{\\sigma}{\\sqrt{n}}\n",
        "$$\n",
        "\n",
        "Here:\n",
        "\n",
        "* $\\bar{x}$ is the sample mean (point estimate),\n",
        "* $\\sigma$ is the population standard deviation,\n",
        "* $n$ is the sample size,\n",
        "* $Z^*$ is the **critical Z-score** corresponding to the desired confidence level.\n",
        "\n",
        "The **Z-score $Z^*$** represents how many standard deviations away from the mean you must go to capture the middle portion of the normal distribution associated with the confidence level. For example:\n",
        "\n",
        "* For a 90% confidence interval, $Z^* \\approx 1.645$,\n",
        "* For a 95% confidence interval, $Z^* \\approx 1.96$,\n",
        "* For a 99% confidence interval, $Z^* \\approx 2.576$.\n",
        "\n",
        "Thus, the Z-score directly controls the **margin of error** in the confidence interval. A higher confidence level requires a larger Z-score, resulting in a wider interval that is more likely to contain the true parameter.\n",
        "\n"
      ],
      "metadata": {
        "id": "bNU-jHDtGMSb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How are Z-scores used to compare different distributions?\n",
        "\n",
        "- **Z-scores** are used to compare different distributions by **standardizing** data points from each distribution onto a common scale. Since different distributions can have different means and standard deviations, raw values alone aren‚Äôt directly comparable. But by converting values into Z-scores, you express how many standard deviations each value is from its own distribution‚Äôs mean.\n",
        "\n",
        "For example, if you have test scores from two different exams with different averages and spreads, calculating the Z-score for a student‚Äôs score on each exam allows you to see which performance is better relative to the respective group. A higher Z-score means the score is further above the mean in terms of standard deviations, regardless of the original scale or units.\n",
        "\n",
        "This standardization enables meaningful comparisons across different datasets or variables, making Z-scores a powerful tool for evaluating relative standing or performance across varied contexts.\n"
      ],
      "metadata": {
        "id": "L_lKYyJTGT3T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What are the assumptions for applying the Central Limit Theorem?\n",
        "\n",
        "- The **Central Limit Theorem (CLT)** relies on a few key assumptions to hold true when approximating the distribution of sample means as normal:\n",
        "\n",
        "1. **Independence:** The individual observations in the sample must be independent of each other. This usually means the data should come from a random sample or randomized experiment, and when sampling without replacement, the sample size should be less than about 10% of the population to reduce dependence.\n",
        "\n",
        "2. **Sample Size:** The sample size $n$ should be sufficiently large. While \"large enough\" can vary, a common rule of thumb is $n \\geq 30$. For populations that are strongly skewed or have heavy tails, larger samples may be needed for the CLT to apply well.\n",
        "\n",
        "3. **Identically Distributed:** The observations should come from the same population distribution (i.e., the data should be identically distributed), ensuring each observation has the same underlying distribution.\n",
        "\n",
        "When these assumptions are met, the distribution of the sample mean tends toward a normal distribution as the sample size increases, regardless of the original population‚Äôs shape.\n"
      ],
      "metadata": {
        "id": "7mXFeRTwGhUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. What is the concept of expected value in a probability distribution?\n",
        "\n",
        "- The **expected value** of a probability distribution, often denoted as $E[X]$ or $\\mu$, is the **long-run average or mean value** you would expect to observe if you repeated a random experiment many times. It represents the **weighted average of all possible outcomes**, where each outcome is weighted by its probability.\n",
        "\n",
        "For a **discrete random variable** $X$ with possible values $x_i$ and probabilities $p_i = P(X = x_i)$, the expected value is calculated as:\n",
        "\n",
        "$$\n",
        "E[X] = \\sum_i x_i \\cdot p_i\n",
        "$$\n",
        "\n",
        "For a **continuous random variable** with probability density function $f(x)$, it is given by:\n",
        "\n",
        "$$\n",
        "E[X] = \\int_{-\\infty}^{\\infty} x \\cdot f(x) \\, dx\n",
        "$$\n",
        "\n",
        "The expected value provides a measure of the central tendency of the distribution, telling you where the \"center of mass\" of the probability is located. It‚Äôs a fundamental concept in probability and statistics because it helps summarize the average outcome of a random process.\n"
      ],
      "metadata": {
        "id": "RW3V0SzdGsxK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23.  How does a probability distribution relate to the expected outcome of a random variable?\n",
        "\n",
        "- A **probability distribution** describes all possible outcomes of a random variable and the likelihood (probability) of each outcome occurring. The **expected outcome** (or expected value) of the random variable is essentially a **summary measure** derived from this distribution‚Äîit represents the **average or mean value** you would expect if the random experiment were repeated many times.\n",
        "\n",
        "Specifically, the expected value is calculated by weighting each possible outcome by its probability (in the case of discrete variables) or by integrating over the range of possible values multiplied by their probability density (for continuous variables). This ties the expected outcome directly to the probability distribution because the distribution determines both the possible values and their associated probabilities, which together define what the average or ‚Äúexpected‚Äù result will be.\n",
        "\n",
        "In other words, the expected value is the **center of gravity** of the probability distribution, showing where the outcomes tend to balance out on average.\n"
      ],
      "metadata": {
        "id": "nyPArhM3G1Xq"
      }
    }
  ]
}